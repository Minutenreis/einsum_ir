import tvm
import tvm.auto_scheduler
import tvm_helper
import argparse
##
# Note: This script was automatically generated by tvm_from_tree.py.
#
# input_string: [[[4,9,0]->[0,4,9]],[[[0,5,1]->[5,1,0]],[[1,6,2]->[6,2,1]]->[5,6,2,0]]->[5,6,2,4,9]],[[[3,8,4]->[8,3,4]],[[2,7,3]->[7,2,3]]->[7,2,8,4]]->[5,6,7,8,9]
##
@tvm.auto_scheduler.register_workload
def einsum_tree( dim_0, dim_1, dim_2, dim_3, dim_4, dim_5, dim_6, dim_7, dim_8, dim_9, dtype):
  tensor_3_8_4 = tvm.te.placeholder((dim_3, dim_8, dim_4), name='tensor_3_8_4', dtype=dtype)
  tensor_2_7_3 = tvm.te.placeholder((dim_2, dim_7, dim_3), name='tensor_2_7_3', dtype=dtype)
  tensor_0_5_1 = tvm.te.placeholder((dim_0, dim_5, dim_1), name='tensor_0_5_1', dtype=dtype)
  tensor_1_6_2 = tvm.te.placeholder((dim_1, dim_6, dim_2), name='tensor_1_6_2', dtype=dtype)
  tensor_4_9_0 = tvm.te.placeholder((dim_4, dim_9, dim_0), name='tensor_4_9_0', dtype=dtype)

  tmp_4 = tvm.te.reduce_axis((0, dim_4), name='tmp_4')
  tmp_1 = tvm.te.reduce_axis((0, dim_1), name='tmp_1')
  tmp_3 = tvm.te.reduce_axis((0, dim_3), name='tmp_3')
  tmp_0 = tvm.te.reduce_axis((0, dim_0), name='tmp_0')
  tmp_2 = tvm.te.reduce_axis((0, dim_2), name='tmp_2')

  tensor_7_2_8_4 = tvm.te.compute( (dim_7, dim_2, dim_8, dim_4), lambda tmp_7, tmp_2, tmp_8, tmp_4: tvm.te.sum( tensor_3_8_4[ tmp_3, tmp_8, tmp_4 ] * tensor_2_7_3[ tmp_2, tmp_7, tmp_3 ] , axis=[ tmp_3 ]), name='tensor_7_2_8_4' )
  tensor_5_6_2_0 = tvm.te.compute( (dim_5, dim_6, dim_2, dim_0), lambda tmp_5, tmp_6, tmp_2, tmp_0: tvm.te.sum( tensor_0_5_1[ tmp_0, tmp_5, tmp_1 ] * tensor_1_6_2[ tmp_1, tmp_6, tmp_2 ] , axis=[ tmp_1 ]), name='tensor_5_6_2_0' )
  tensor_5_6_2_4_9 = tvm.te.compute( (dim_5, dim_6, dim_2, dim_4, dim_9), lambda tmp_5, tmp_6, tmp_2, tmp_4, tmp_9: tvm.te.sum( tensor_4_9_0[ tmp_4, tmp_9, tmp_0 ] * tensor_5_6_2_0[ tmp_5, tmp_6, tmp_2, tmp_0 ] , axis=[ tmp_0 ]), name='tensor_5_6_2_4_9' )
  tensor_5_6_7_8_9 = tvm.te.compute( (dim_5, dim_6, dim_7, dim_8, dim_9), lambda tmp_5, tmp_6, tmp_7, tmp_8, tmp_9: tvm.te.sum( tensor_5_6_2_4_9[ tmp_5, tmp_6, tmp_2, tmp_4, tmp_9 ] * tensor_7_2_8_4[ tmp_7, tmp_2, tmp_8, tmp_4 ] , axis=[ tmp_4, tmp_2 ]), name='tensor_5_6_7_8_9' )

  return [ tensor_3_8_4, tensor_2_7_3, tensor_0_5_1, tensor_1_6_2, tensor_4_9_0, tensor_5_6_7_8_9 ]

if __name__=="__main__":
  args = tvm_helper.parse_args()

  target = tvm.target.Target( tvm_helper.cpu_to_llvm( args.cpu ) )
  hardware_params = tvm.auto_scheduler.HardwareParams( target = target )
  dtype = args.dtype
  num_measure_trials = args.num_measure_trials
  timeout = args.timeout
  log_file = args.log_file

  einsum_str = "DIE,CHD,AFB,BGC,EJA->FGHIJ"
  func = einsum_tree
  sizes = (40, 40, 40, 40, 40, 25, 25, 25, 25, 25)

  tvm_helper.run_all( einsum_str,
                      func,
                      sizes,
                      dtype,
                      hardware_params,
                      target,
                      num_measure_trials,
                      timeout,
                      log_file )
