import tvm
import tvm.auto_scheduler
import tvm_helper
import argparse
##
# Note: This script was automatically generated by tvm_from_tree.py.
#
# input_string: [[[[[3,7,4,11]->[4,11,7,3]],[[2,6,7,10]->[6,10,2,7]]->[6,4,10,11,2,3]],[[8,9,10,11]->[9,8,10,11]]->[9,6,8,4,2,3]],[[0,4,5,8]->[0,5,8,4]]->[0,9,5,6,2,3]],[[1,5,6,9]->[1,9,5,6]]->[0,1,2,3]
##
@tvm.auto_scheduler.register_workload
def einsum_tree( dim_0, dim_1, dim_2, dim_3, dim_4, dim_5, dim_6, dim_7, dim_8, dim_9, dim_10, dim_11, dtype):
  tensor_3_7_4_11 = tvm.te.placeholder((dim_3, dim_7, dim_4, dim_11), name='tensor_3_7_4_11', dtype=dtype)
  tensor_2_6_7_10 = tvm.te.placeholder((dim_2, dim_6, dim_7, dim_10), name='tensor_2_6_7_10', dtype=dtype)
  tensor_1_5_6_9 = tvm.te.placeholder((dim_1, dim_5, dim_6, dim_9), name='tensor_1_5_6_9', dtype=dtype)
  tensor_0_4_5_8 = tvm.te.placeholder((dim_0, dim_4, dim_5, dim_8), name='tensor_0_4_5_8', dtype=dtype)
  tensor_8_9_10_11 = tvm.te.placeholder((dim_8, dim_9, dim_10, dim_11), name='tensor_8_9_10_11', dtype=dtype)

  tmp_11 = tvm.te.reduce_axis((0, dim_11), name='tmp_11')
  tmp_5 = tvm.te.reduce_axis((0, dim_5), name='tmp_5')
  tmp_6 = tvm.te.reduce_axis((0, dim_6), name='tmp_6')
  tmp_4 = tvm.te.reduce_axis((0, dim_4), name='tmp_4')
  tmp_10 = tvm.te.reduce_axis((0, dim_10), name='tmp_10')
  tmp_8 = tvm.te.reduce_axis((0, dim_8), name='tmp_8')
  tmp_7 = tvm.te.reduce_axis((0, dim_7), name='tmp_7')
  tmp_9 = tvm.te.reduce_axis((0, dim_9), name='tmp_9')

  tensor_6_4_10_11_2_3 = tvm.te.compute( (dim_6, dim_4, dim_10, dim_11, dim_2, dim_3), lambda tmp_6, tmp_4, tmp_10, tmp_11, tmp_2, tmp_3: tvm.te.sum( tensor_3_7_4_11[ tmp_3, tmp_7, tmp_4, tmp_11 ] * tensor_2_6_7_10[ tmp_2, tmp_6, tmp_7, tmp_10 ] , axis=[ tmp_7 ]), name='tensor_6_4_10_11_2_3' )
  tensor_9_6_8_4_2_3 = tvm.te.compute( (dim_9, dim_6, dim_8, dim_4, dim_2, dim_3), lambda tmp_9, tmp_6, tmp_8, tmp_4, tmp_2, tmp_3: tvm.te.sum( tensor_6_4_10_11_2_3[ tmp_6, tmp_4, tmp_10, tmp_11, tmp_2, tmp_3 ] * tensor_8_9_10_11[ tmp_8, tmp_9, tmp_10, tmp_11 ] , axis=[ tmp_11, tmp_10 ]), name='tensor_9_6_8_4_2_3' )
  tensor_0_9_5_6_2_3 = tvm.te.compute( (dim_0, dim_9, dim_5, dim_6, dim_2, dim_3), lambda tmp_0, tmp_9, tmp_5, tmp_6, tmp_2, tmp_3: tvm.te.sum( tensor_9_6_8_4_2_3[ tmp_9, tmp_6, tmp_8, tmp_4, tmp_2, tmp_3 ] * tensor_0_4_5_8[ tmp_0, tmp_4, tmp_5, tmp_8 ] , axis=[ tmp_4, tmp_8 ]), name='tensor_0_9_5_6_2_3' )
  tensor_0_1_2_3 = tvm.te.compute( (dim_0, dim_1, dim_2, dim_3), lambda tmp_0, tmp_1, tmp_2, tmp_3: tvm.te.sum( tensor_0_9_5_6_2_3[ tmp_0, tmp_9, tmp_5, tmp_6, tmp_2, tmp_3 ] * tensor_1_5_6_9[ tmp_1, tmp_5, tmp_6, tmp_9 ] , axis=[ tmp_5, tmp_9, tmp_6 ]), name='tensor_0_1_2_3' )

  return [ tensor_3_7_4_11, tensor_2_6_7_10, tensor_1_5_6_9, tensor_0_4_5_8, tensor_8_9_10_11, tensor_0_1_2_3 ]

if __name__=="__main__":
  args = tvm_helper.parse_args()

  target = tvm.target.Target( tvm_helper.cpu_to_llvm( args.cpu ) )
  hardware_params = tvm.auto_scheduler.HardwareParams( target = target )
  dtype = args.dtype
  num_measure_trials = args.num_measure_trials
  timeout = args.timeout
  log_file = args.log_file

  einsum_str = "DHEL,CGHK,BFGJ,AEFI,IJKL->ABCD"
  func = einsum_tree
  sizes = (40, 40, 20, 20, 6, 6, 6, 6, 4, 4, 4, 4)

  tvm_helper.run_all( einsum_str,
                      func,
                      sizes,
                      dtype,
                      hardware_params,
                      target,
                      num_measure_trials,
                      timeout,
                      log_file )
