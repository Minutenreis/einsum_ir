import tvm
import tvm.auto_scheduler
import tvm_helper
import argparse
##
# Note: This script was automatically generated by tvm_from_tree.py.
#
# input_string: [[[[[8,13,14]->[13,14,8]],[4,12,13]->[4,12,14,8]],[[3,11,12],[2,9,11]->[3,2,9,12]]->[3,2,4,9,14,8]],[[[[7,14,15]->[7,15,14]],[[6,15,16]->[6,16,15]]->[6,7,16,14]],[[5,10,16],[1,9,10]->[5,1,9,16]]->[5,1,9,6,7,14]]->[5,1,3,2,4,6,7,8]],[[0,1,2,3,4]->[1,0,3,2,4]]->[0,5,6,7,8]
##
@tvm.auto_scheduler.register_workload
def einsum_tree( dim_0, dim_1, dim_2, dim_3, dim_4, dim_5, dim_6, dim_7, dim_8, dim_9, dim_10, dim_11, dim_12, dim_13, dim_14, dim_15, dim_16, dtype):
  tensor_5_10_16 = tvm.te.placeholder((dim_5, dim_10, dim_16), name='tensor_5_10_16', dtype=dtype)
  tensor_1_9_10 = tvm.te.placeholder((dim_1, dim_9, dim_10), name='tensor_1_9_10', dtype=dtype)
  tensor_7_14_15 = tvm.te.placeholder((dim_7, dim_14, dim_15), name='tensor_7_14_15', dtype=dtype)
  tensor_6_15_16 = tvm.te.placeholder((dim_6, dim_15, dim_16), name='tensor_6_15_16', dtype=dtype)
  tensor_3_11_12 = tvm.te.placeholder((dim_3, dim_11, dim_12), name='tensor_3_11_12', dtype=dtype)
  tensor_2_9_11 = tvm.te.placeholder((dim_2, dim_9, dim_11), name='tensor_2_9_11', dtype=dtype)
  tensor_8_13_14 = tvm.te.placeholder((dim_8, dim_13, dim_14), name='tensor_8_13_14', dtype=dtype)
  tensor_4_12_13 = tvm.te.placeholder((dim_4, dim_12, dim_13), name='tensor_4_12_13', dtype=dtype)
  tensor_0_1_2_3_4 = tvm.te.placeholder((dim_0, dim_1, dim_2, dim_3, dim_4), name='tensor_0_1_2_3_4', dtype=dtype)

  tmp_10 = tvm.te.reduce_axis((0, dim_10), name='tmp_10')
  tmp_4 = tvm.te.reduce_axis((0, dim_4), name='tmp_4')
  tmp_16 = tvm.te.reduce_axis((0, dim_16), name='tmp_16')
  tmp_3 = tvm.te.reduce_axis((0, dim_3), name='tmp_3')
  tmp_12 = tvm.te.reduce_axis((0, dim_12), name='tmp_12')
  tmp_9 = tvm.te.reduce_axis((0, dim_9), name='tmp_9')
  tmp_11 = tvm.te.reduce_axis((0, dim_11), name='tmp_11')
  tmp_15 = tvm.te.reduce_axis((0, dim_15), name='tmp_15')
  tmp_14 = tvm.te.reduce_axis((0, dim_14), name='tmp_14')
  tmp_13 = tvm.te.reduce_axis((0, dim_13), name='tmp_13')
  tmp_2 = tvm.te.reduce_axis((0, dim_2), name='tmp_2')
  tmp_1 = tvm.te.reduce_axis((0, dim_1), name='tmp_1')

  tensor_5_1_9_16 = tvm.te.compute( (dim_5, dim_1, dim_9, dim_16), lambda tmp_5, tmp_1, tmp_9, tmp_16: tvm.te.sum( tensor_5_10_16[ tmp_5, tmp_10, tmp_16 ] * tensor_1_9_10[ tmp_1, tmp_9, tmp_10 ] , axis=[ tmp_10 ]), name='tensor_5_1_9_16' )
  tensor_6_7_16_14 = tvm.te.compute( (dim_6, dim_7, dim_16, dim_14), lambda tmp_6, tmp_7, tmp_16, tmp_14: tvm.te.sum( tensor_7_14_15[ tmp_7, tmp_14, tmp_15 ] * tensor_6_15_16[ tmp_6, tmp_15, tmp_16 ] , axis=[ tmp_15 ]), name='tensor_6_7_16_14' )
  tensor_5_1_9_6_7_14 = tvm.te.compute( (dim_5, dim_1, dim_9, dim_6, dim_7, dim_14), lambda tmp_5, tmp_1, tmp_9, tmp_6, tmp_7, tmp_14: tvm.te.sum( tensor_6_7_16_14[ tmp_6, tmp_7, tmp_16, tmp_14 ] * tensor_5_1_9_16[ tmp_5, tmp_1, tmp_9, tmp_16 ] , axis=[ tmp_16 ]), name='tensor_5_1_9_6_7_14' )
  tensor_3_2_9_12 = tvm.te.compute( (dim_3, dim_2, dim_9, dim_12), lambda tmp_3, tmp_2, tmp_9, tmp_12: tvm.te.sum( tensor_3_11_12[ tmp_3, tmp_11, tmp_12 ] * tensor_2_9_11[ tmp_2, tmp_9, tmp_11 ] , axis=[ tmp_11 ]), name='tensor_3_2_9_12' )
  tensor_4_12_14_8 = tvm.te.compute( (dim_4, dim_12, dim_14, dim_8), lambda tmp_4, tmp_12, tmp_14, tmp_8: tvm.te.sum( tensor_8_13_14[ tmp_8, tmp_13, tmp_14 ] * tensor_4_12_13[ tmp_4, tmp_12, tmp_13 ] , axis=[ tmp_13 ]), name='tensor_4_12_14_8' )
  tensor_3_2_4_9_14_8 = tvm.te.compute( (dim_3, dim_2, dim_4, dim_9, dim_14, dim_8), lambda tmp_3, tmp_2, tmp_4, tmp_9, tmp_14, tmp_8: tvm.te.sum( tensor_4_12_14_8[ tmp_4, tmp_12, tmp_14, tmp_8 ] * tensor_3_2_9_12[ tmp_3, tmp_2, tmp_9, tmp_12 ] , axis=[ tmp_12 ]), name='tensor_3_2_4_9_14_8' )
  tensor_5_1_3_2_4_6_7_8 = tvm.te.compute( (dim_5, dim_1, dim_3, dim_2, dim_4, dim_6, dim_7, dim_8), lambda tmp_5, tmp_1, tmp_3, tmp_2, tmp_4, tmp_6, tmp_7, tmp_8: tvm.te.sum( tensor_3_2_4_9_14_8[ tmp_3, tmp_2, tmp_4, tmp_9, tmp_14, tmp_8 ] * tensor_5_1_9_6_7_14[ tmp_5, tmp_1, tmp_9, tmp_6, tmp_7, tmp_14 ] , axis=[ tmp_9, tmp_14 ]), name='tensor_5_1_3_2_4_6_7_8' )
  tensor_0_5_6_7_8 = tvm.te.compute( (dim_0, dim_5, dim_6, dim_7, dim_8), lambda tmp_0, tmp_5, tmp_6, tmp_7, tmp_8: tvm.te.sum( tensor_5_1_3_2_4_6_7_8[ tmp_5, tmp_1, tmp_3, tmp_2, tmp_4, tmp_6, tmp_7, tmp_8 ] * tensor_0_1_2_3_4[ tmp_0, tmp_1, tmp_2, tmp_3, tmp_4 ] , axis=[ tmp_1, tmp_4, tmp_3, tmp_2 ]), name='tensor_0_5_6_7_8' )

  return [ tensor_5_10_16, tensor_1_9_10, tensor_7_14_15, tensor_6_15_16, tensor_3_11_12, tensor_2_9_11, tensor_8_13_14, tensor_4_12_13, tensor_0_1_2_3_4, tensor_0_5_6_7_8 ]

if __name__=="__main__":
  args = tvm_helper.parse_args()

  target = tvm.target.Target( tvm_helper.cpu_to_llvm( args.cpu ) )
  hardware_params = tvm.auto_scheduler.HardwareParams( target = target )
  dtype = args.dtype
  num_measure_trials = args.num_measure_trials
  timeout = args.timeout
  log_file = args.log_file

  einsum_str = "FKQ,BJK,HOP,GPQ,DLM,CJL,INO,EMN,ABCDE->AFGHI"
  func = einsum_tree
  sizes = (128, 4, 7, 4, 7, 3, 4, 5, 5, 50, 50, 50, 50, 50, 50, 50, 50)

  tvm_helper.run_all( einsum_str,
                      func,
                      sizes,
                      dtype,
                      hardware_params,
                      target,
                      num_measure_trials,
                      timeout,
                      log_file )
