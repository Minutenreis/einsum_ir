\section{Background}

\subsection{Tensorcontractions}
\label{Tensorcontractions}

%TODO: HOW THE HELL DO I INTRODUCE MATH?!
Tensorcontractions are a generalization from batched matrix multiplications on n-dimensional Arrays called order-n tensors.
Imagine contraction of an order-$n$ tensor $A \in \mathbb{R}^{I_1\times\dots\times I_n}$ and an order-$m$ tensor $B \in \mathbb{R}^{J_1\times\dots\times J_m}$ with k shared dimensions to multiply to a new order-$(n+m-k)$ tensor $C \in \mathbb{R}^{K^1\times\dots\times K^{(n+m-k)}}$.
As an example from "Einsum is all you need" \cite{einsum_is_all_you_need} imagine $n=4, m=5$, $I_2 = J_3$ and $I_3 = J_5$.
The resulting Tensor $C$ would be computed as $C_{pstuv}=\sum_{q}\sum_{r}A_{pqrs}B_{tuqvr}$.

\subsection{Einsum expressions}

Einsumexpressions allow for a more succinct expression than the typical Tensor notation as used in \ref{Tensorcontractions}.
Instead of writing $C_{pstuv}=\sum_{q}\sum_{r}A_{pqrs}B_{tuqvr}$ the same contraction is expressed as $A_{pqrs}B_{tuqvr} \rightarrow C_{pstuv}$.
The summation signs are now implicit.
Einsumexpressions can also describe the contractions from more than 2 tensors\cite{einsum_is_all_you_need}.
Instead of writing $D_{ij} = \sum_{k}\sum_{l}A_{ik}B_{jkl}C_{il}$ it is expressed as $A_{ik}B_{jkl}C_{il} \rightarrow D_{ij}$.
The evaluation of einsum expression is supported in the popular frameworks Torch\cite{torch}, PyTorch\cite{pytorch} and Tensorflow\cite{tensorflow}.

\subsection{einsum\_ir}

\texttt{einsum\_ir}\cite{einsum_ir} is a software library developed at the Friedrich Schiller University to accelerate a series of einsum expressions in C++.
My work will build on top of the existing work done in this library.
Specifically I will use their implementation of a binary tensor contraction $A,B \rightarrow C$ as local primitive for my algorithm.
\texttt{einsum\_ir} already implements shared memory parallelisation with OpenMP\cite{openMP}, so it can already exploit all cpu cores on a single NUMA domain.
Its performance is currently not scaling well across more than one cpu. %todo: cite or test?
This thesis provides algorithms aimed at a secondary distributed memory parallelization to alleviate that issue.
An important note for the current binary tensor contraction interface is that it expects all tensors, both input and output, to be contiguous in memory.
