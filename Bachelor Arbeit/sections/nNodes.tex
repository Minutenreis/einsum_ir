\section{Scaling across arbitrary number of Nodes}

The previously discussed Master-Worker algorithms expect all the data before and after a contraction to reside within 1 node.
For large tensors this is no longer feasable, as scratch memory on a node is limited and using main storage is prohibitively slow.
The communication load is also very unbalanced for more than 2 nodes, where the master has to communicate with all other nodes while each node only has to communicate with the master.
To circumvent all these problems the next algorithms will keep the data distributed in the whole contraction.
This needs both less memory on each node and allows very balanced communication loads.

As basis for the following algorithms we make a few assumptions:
\begin{enumerate}
    \item tensors are predistributed across all nodes 
    \item gathering the result tensor is not needed
    \item tensors are sufficiently large and compute intensive that data communication will not be the bottle neck
    \item tensors can be split evenly across ranks
\end{enumerate}

To justify point 1 and 2 we consider our examples as part of a larger Einsumtree that gets contracted.
If the tree is large enough, the initial distribution and the final gatherering of tensors will be negligble compared to the total runtime.
If point 3 is not met the tree should be run on less nodes.
If the tree is sufficiently small just one node and no distributed memory parallelism whatsoever might be better suited towards the problem.
Point 4 is assumed so the algorithms can be described in a simpler manner.
If the system proposed is to be deployed as more than just this proof of concept, one could remedy that requirement by zero padding input tensors or adjusting the algorithms to work on two sizes of each tensor instead (one for the fully filled chunks and one for the partial chunks).

With these assumptions met the main goal for the algorithms will be to increase throughput as much as possible.
To achieve that each node has to compute ideally the entire runtime of a contraction, which necessitates overlapping communication and computation (should communication occur).
The following algorithms will employ an extra communication thread to achieve this overlap.
This work proposes 3 algorithms for the following scenarios:
\begin{enumerate}
    \item in both input tensors the same c dimension is split
    \item in both input tensors a m/n dimension is split
    \item in both input tensors the same k dimension is split and one tensors outer most dimension is m/n
\end{enumerate}

\subsection{distributed c dimension algorithm}

\begin{figure}[h]
\centering\includegraphics[width=0.3\textwidth]{dist_c.pdf}
\caption{Visualization of the distributed c algorithm; 
each color represents one node with the coloured blocks representing the data they hold; 
since the tensors are distributed along their c dimension, each node can locally contract its chunks to generate their chunk of the output tensor}
\label{fig:c_algo}
\end{figure}

Distribution along a c dimension is the simplest algorithm we consider. 
As seen in Figure \ref{fig:c_algo} we assume both input tensors are distributed along the same c dimension.
We also generally expect all distributions to follow the same order within ranks, meaning that rank 0 always keeps the first chunk, rank 1 the second and so on.
If now both input chunks are contracted on each node, the result will be a chunk of the output tensor distributed along the same c dimension.
At least locally this is optimal, since all nodes have the same workload and no additional communication has to occur.
As long as a tensor can be split among all nodes, this algorithm is preferable to the non distributed case.

\subsection{distributed m/n dimensions algorithm}

\begin{figure}[h]
\centering{
\includegraphics[width=0.75\textwidth]{dist_m_n.pdf}
\includegraphics[width=0.2\textwidth]{ring.pdf}
}
\caption{Visualization of the distributed m/n algorithm; 
each color represents one node with the coloured blocks representing the data they hold; 
since the tensors are split along their m/n dimensions each node can only locally contract $\frac{1}{totalNodes}$th part of the output tensor.
To generate a full chunk of the output tensor, each node contracts its current right tensor with their left tensor while simultaneously sending it to their previous neighbour in the ring and receiving their next right tensor from their next neighbour.
After $totalNodes$ iterations this will result in an output tensor that is distributed along the same dimension as the left tensor.
In the data movement is visualized with arrows of each nodes color pointing to the next right tensor they receive.
The ring communication used is depicted on the right, where each node can only communicate with its immediate neighbours.
}
\label{fig:m_n_algo}
\end{figure}

For the communication of this algorithm we assume that all nodes are set up in a ring as seen in the right part of Figure \ref{fig:m_n_algo}.
If both input tensors are distributed alongside their m/n dimensions, communication between nodes is unavoidable.
In contrast to the distributed c algorithm, each node can only calculate a $\frac{1}{totalNodes}$ big chunk of  its output tensor locally.
Instead of gathering all the data needed to calculate the full chunk of the output tensor, we'll proceed in $\frac{1}{totalNodes}$ subchunks.
This has the advantage that we can calculate the first output chunk immediately, so we have no startup latency to hide.
This output chunk will be offset based on the rank of each node, as seen in Figure \ref{fig:m_n_algo}.
While each node contracts its first chunk, they also each send their currently contracting right chunk towards the previous node in the ring.
To achieve that each node will use an extra communication thread so the communication gets overlapped with the contraction and allocate an extra buffer for the receiving tensor to go into.
After each step the receiving and calculating tensors switch their functions.
After $totalNodes$ iterations each node has one chunk of the output tensor that is distributed along the same dimension as the left input tensor.
As long as the time to communicate one chunk of the right tensor is not greater than the time to contract one subchunk, this will hide all communication times from the final contraction time.

At this point a few design considerations should be noted:\\
\textbf{The implemented algorithm omits communicating tensors in the last iteration.}
This last communication is not needed since there is no contraction following.
It would only be necessary if the input tensors should finish in the same state as they started.
This is currently unneeded as only trees are considered as input and no directed acyclic graphs (DAG's).
In a tree each input gets only used exactly once, so the input tensors will get freed after the calculation anyways.
Should the algorithm be adjusted to also work for DAG's this last communication would have to occur.
Should an uneven number of nodes be employed that last step would also need to be followed by a local copy from the allocated buffer to the original input tensor.\\
\textbf{Ring communication is chosen to reduce additional memory needed.}
If alternatively each node keeps its input tensors to send to each other node directly, it would need another 2 buffers, one for the currently contracted tensor (except in the first iteration where it already has that buffer in the form of the input tensor) and another for the tensor it currently receives.
The size of these buffers could be reduced by chosing a smaller subchunk size, but that would involve more smaller MPI packages and smaller contractions, both of which are worse for performance.
Also the size of the buffers in the ring algorithm could be reduced in a similar manner and still require only half of the additional memory for the same subchunk size.\\
\textbf{Due to a limitation in the current contraction interface, the distributed dimension of the right tensor has to be the outermost dimension of the output tensor.}
This is necessary since the current interface supports only the contractions of two contiguous chunks of memory into a third contiguous chunk.
To divide the output tensor into $totalNodes$ many contiguous chunks the split dimension of the subchunks needs to be the outer most dimension.
This limitation could be remedied by expanding the contraction interface to support strided input / output tensors.
Since the remedy is simple the limitation will be ignored in the rest of this thesis.
For time reasons this expansion of the interface was not explored.\\
\textbf{A seperate algorithm for keeping the distributed dimension of the right tensor is not needed.}
This algorithm would work completely analogously to the m/n algorithm depicted, but communicate the left tensor across nodes instead.
To achieve the same result the two contracted tensors may be swapped in the tree without any restriction, which achieves the same result.


%todo: write why interleaving multiple timesteps was not explored (time constraints, would help if nodes have irregular breaks for example)
%todo: where should I describe why I implement tensor parallelism instead of pipeline parallelism (not suited if tree is relatively flat) or "tree" parallelism (not suited if tree is relatively deep)
%todo: use einsum notation somewhere (?)
%todo: should I describe here why I only expect 1 dimension to be split (1 split into 4 instead of 2 into 2 each for example)

\subsection{distributed k dimension algorithm}

\begin{figure}[h]
    \centering{
    \includegraphics[width=0.5\textwidth]{dist_k.pdf}
    \includegraphics[width=0.34\textwidth]{dist_k_memory.pdf}
    }
    \caption{Visualization of the distributed k algorithm; 
    each color represents one node with the coloured blocks representing the data they hold; 
    In contrast to the m/n algorithm in Figure \ref{fig:m_n_algo}, this algorithm moves the output tensor through the nodes.
    The left graphic depicts the data movement from the perspective of rank 0 for 4 nodes.
    It starts with the chunk of the next rank and then linearly adds its part of the contraction to each, ending with its own chunk of the output tensor, at which point the contraction completes.
    The right graphic depicts the memory layout used, with two contiguous chunks being stored in the output tensor and one extra chunk as additional memory.
    After each timestep the function of each tensor rotates ($calc \rightarrow send \rightarrow recv \rightarrow calc$).
    After three timesteps each tensor has the same function.
    To end with its whole chunk in the output tensor the last timestep has to use the bottom left format.
    }
    \label{fig:k_algo}
\end{figure}

