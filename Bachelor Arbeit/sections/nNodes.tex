\section{Scaling across arbitrary number of Nodes}

The previously discussed Master-Worker algorithms expect all the data before and after a contraction to reside within 1 node.
For large tensors this is no longer feasable, as scratch memory on a node is limited and using main storage is prohibitively slow.
The communication load is also very unbalanced for more than 2 nodes, where the master has to communicate with all other nodes while each node only has to communicate with the master.
To circumvent all these problems the next algorithms will keep the data distributed in the whole contraction.
This needs both less memory on each node and allows very balanced communication loads.

As basis for the following algorithms we make a few assumptions:
\begin{enumerate}
    \item tensors are predistributed across all nodes 
    \item gathering the result tensor is not needed
    \item tensors are sufficiently large and compute intensive that data communication will not be the bottle neck
    \item tensors can be split evenly across ranks
\end{enumerate}

To justify point 1 and 2 we consider our examples as part of a larger Einsumtree that gets contracted.
If the tree is large enough, the initial distribution and the final gatherering of tensors will be negligble compared to the total runtime.
If point 3 is not met the tree should be run on less nodes.
If the tree is sufficiently small just one node and no distributed memory parallelism whatsoever might be better suited towards the problem.
Point 4 is assumed so the algorithms can be described in a simpler manner.
If the system proposed is to be deployed as more than just this proof of concept, one could remedy that requirement by zero padding input tensors or adjusting the algorithms to work on two sizes of each tensor instead (one for the fully filled chunks and one for the partial chunks).

With these assumptions met the main goal for the algorithms will be to increase throughput as much as possible.
To achieve that each node has to compute ideally the entire runtime of a contraction, which necessitates overlapping communication and computation (should communication occur).
The following algorithms will employ an extra communication thread to achieve this overlap.
This work proposes 3 algorithms for the following scenarios:
\begin{enumerate}
    \item in both input tensors the same c dimension is split
    \item in both input tensors a m/n dimension is split
    \item in both input tensors the same k dimension is split and one tensors outer most dimension is m/n
\end{enumerate}

\subsection{distributed c dimension algorithm}

\begin{figure}[h]
\centering\includegraphics[width=0.3\textwidth]{dist_c.pdf}
\caption{Visualization of the distributed c algorithm; 
each color represents one node with the coloured blocks representing the data they hold; 
if the tensors are distributed along a c dimension no communication has to occur and each node just contracts its local input tensors to generate its chunk of the output tensor}
\label{fig:c_algo}
\end{figure}

Distribution along a c dimension is the simplest algorithm we consider. 
Since all c dimensions are independent of each other, no communication has to occur between the nodes.
Each node will locally contract its chunks of the input tensors.
The resulting output tensor is thereby distributed along the same dimension as its input tensors were.
This algorithm represents the optimal case for distribution, as no communication overhead can occur.

\subsection{distributed m/n dimensions algorithm}

\begin{figure}[h]
\includegraphics[width=0.79\textwidth]{dist_m_n_out_m.pdf}
\includegraphics[width=0.2\textwidth]{ring.pdf}
\caption{Visualization of the distributed m/n algorithm; 
each color represents one node with the coloured blocks representing the data they hold; 
if the tensors are distributed along their respective m/n dimensions the right tensor will be communicated between all nodes. 
Each node will calculate a chunk of their output tensor while receiving the next input tensor thats needed (visualized with arrows in the colour of the receiving node). 
The right figure shows the nodes setup as ring, where each node may only communicate with their respective next and previous node.}
\label{fig:m_n_algo}
\end{figure}

For the communication of this algorithm we assume that all nodes are set up in a ring as seen in the right part of Figure \ref{fig:m_n_algo}.
If both input tensors are distributed alongside their m/n dimensions, communication between nodes is unavoidable.
In contrast to being distributed along their c dimension, each node can only calculate a $\frac{1}{n^2}$ big chunk of the output tensor locally, where n is the number of nodes employed in the distributed contraction.
Each node needs to end up with a $\frac{1}{n}$ big chunk of the output tensor.
This algorithm achieves this by doing $n$ contractions, where the first $n-1$ contractions are overlapped by each node communicating its currently being contracted right tensor to the previous node in the ring.
In each step $\frac{1}{n}$'th of each nodes chunk of the output tensor is calculated.
Due to a limitation in the contraction interface of the current application, the outer most dimension of the output tensor has to be the distributed dimension of the right tensor, as the interface expects input and output tensors to be contiguous in memory.
This limitation could be circumvented by inputting not just the tensors but also their strides into the interface.
In the rest of this thesis that limitation will be disregarded.
This algorithm only has a communication overhead if the time to send and receive one chunk of the right tensor is larger than the time to contract one such chunk with its chunk of the left tensor.
In the implementation of this algorithm we also need enough additional memory to hold one extra chunk of the right tensor, so each node can simultaneously contract the current right chunk and receive the next chunk.