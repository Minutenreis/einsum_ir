\section{Future Work}

To make this thesis' result impactful enough to include into \texttt{einsum\_ir} there is still work to do.
First would be rewriting \texttt{einsum\_ir}'s binary contraction implementation to accept strided tensors, so many of the limitation on the distributed dimensions vanish.
The algorithms should also be rewritten to decouple the JIT compilation from the execution, so workloads like inference can work more efficiently.
While doing this support for dimensions that are not a multiple of the process number, as described in Section \ref{sec:n_nodes}, should be added.

When this is done there would need to be development work into finding an algorithm decide when to distribute computations and when to keep them local, likely depending on the tensor sizes.
There also needs to be an algorithm to decide which dimensions are best to distribute in any given einsum expression.
An easy version would be to always distribute the outermost dimension of the output and pull those dimensions to the outermost position in the input tensors.
This would also omit the distributed k-dimension algorithm which showed the worst performance in the examples, since the k-dimension would never be the outermost dimension in the output tensor.
Finally, this algorithm and the algorithms to decide when and how to distribute the tensors would need to be implemented into the regular einsum evaluation of \texttt{einsum\_ir}, most likely as an optional component.

Additionally while scaling across nodes is possible with the algorithms described in this thesis, we have not tested how performant they are at that task.
It is likely that an additional pipeline parallelism layer would be ideal to scale across nodes \cite{megatronLM}.
Such an algorithm could take the tensor parallel scaling algorithms from this thesis as primitive for each node, like we took the shared memory parallelization of \texttt{einsum\_ir} as primitive for each socket.