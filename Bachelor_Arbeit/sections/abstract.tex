\thispagestyle{empty} 

\vbox{
    \begin{minipage}[t][0.5\textheight][t]{\textwidth}
      \section*{Abstract}

      Tensor contractions are critical to the efficient execution of machine learning workloads.
      Einsum expressions offer a succinct and declarative way to express tensor contractions.
      \texttt{einsum\_ir}  is a library to compute a series of einsum expressions in tree form  efficiently on a single compute unit, but its scaling across NUMA domains is lacking.
      This thesis proposes four algorithms to accelerate \texttt{einsum\_ir} with distributed memory parallelism to improve the performance on multi socket systems.
      These algorithms outperform the existing shared memory parallelization for large enough tensors on dual socket systems such as an Nvidia Grace CPU Superchip.
    \end{minipage}

    \nointerlineskip
    \begin{minipage}[b][0.5\textheight][t]{\textwidth}
        \begin{otherlanguage}{ngerman}
          \section*{Kurzfassung}
          
          Tensor Kontraktionen sind für die effiziente Ausführung von maschinellem Lernen entscheidend.
          Einsum Ausdrücke bieten dabei eine prägnante und deklarative Möglichkeit, um Tensor Kontraktionen auszudrücken.
          \texttt{einsum\_ir} ist eine Bibliothek um eine Serie an Einsum Ausdrücken in der Form eines Baumes effizient auf einer einzelnen Recheneinheit auszuwerten, aber es skaliert unzureichend gut über NUMA-Domänen hinweg.
          In dieser Arbeit werden vier Algorithmen zur Beschleunigung von \texttt{einsum\_ir} mit verteiltem Speicherparallelität vorgeschlagen, um die Leistung auf Mehr-Sockel-Systemen zu verbessern.
          Die Algorithmen übertreffen die bestehende verteilter Speicher Parallelisierung für ausreichend große Tensoren auf Zwei-Sockel-Systemen wie dem Nvidia Grace CPU Superchip.
          \end{otherlanguage}
    \end{minipage}
}



\thispagestyle{empty} 


